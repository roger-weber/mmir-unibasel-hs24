{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from text\n",
    "\n",
    "We look at the classical method to describe text documents. We will improve these descriptors in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, JSON\n",
    "import urllib.request\n",
    "import os, re\n",
    "from PyPDF2 import PdfReader\n",
    "from unidecode import unidecode\n",
    "from collections import Counter, defaultdict\n",
    "from helpers import *\n",
    "import math\n",
    "import random\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting text from file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"https://dmi.unibas.ch/fileadmin/user_upload/dmi/Studium/Computer_Science/Vorlesungen_HS23/Multimedia_Retrieval/HS24/03_ClassicalTextRetrieval.pdf\"\n",
    "local_filename = 'example.pdf'\n",
    "\n",
    "# unless local file already exists, download the file\n",
    "if not os.path.exists(local_filename):\n",
    "    urllib.request.urlretrieve(uri, local_filename)\n",
    "    print(f\"File downloaded and saved as {local_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_name: str) -> str:\n",
    "    pages = []\n",
    "\n",
    "    def visitor_text(text, cm, tm, fontDict, fontSize):\n",
    "        y = tm[5]\n",
    "        if y > 20 and len(text) > 0:\n",
    "            # replace \\n and multiple spaces (\\s*) with a single space\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            text = re.sub(r'\\[\\d+\\]|➢|•', '', text)\n",
    "            parts.append(text)\n",
    "\n",
    "    # read the PDF and extract all texts (do some post-processing with above function)\n",
    "    reader = PdfReader(file_name)\n",
    "    for page in reader.pages:\n",
    "        parts = []\n",
    "        page.extract_text(visitor_text=visitor_text)\n",
    "        pages.append(re.sub(r'\\s+',' ', \" \".join(parts)).strip())\n",
    "\n",
    "    # merge text blocks and clean-up\n",
    "    return pages\n",
    "\n",
    "pages = extract_text_from_pdf(local_filename)\n",
    "text = re.sub(r'\\s+',' ', \" \".join(pages))\n",
    "display(Markdown(text[0:4000]+\"...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    text = re.sub(r'[^\\w\\-]+', ' ', text)\n",
    "    tokens = []\n",
    "    for token in text.split(' '):\n",
    "        token = unidecode(token.strip().lower())\n",
    "        if len(token) < 2: continue\n",
    "        if not(re.match(r'^[a-zA-Z][\\w\\-\\.]*$', token)): continue\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize(text)\n",
    "print(\"\\n\".join(tokens[0:20]))\n",
    "print(f'...\\n\\nextracted {len(tokens)} tokens from text with {len(set(tokens))} unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see which terms appear most often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(text)\n",
    "print_table(Counter(tokens).most_common(20),['token', 'frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply porter stemming to reduce words to a common stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def reduce_to_stems(tokens):\n",
    "    return list(map(lambda token: porter_stemmer.stem(token), tokens))\n",
    "\n",
    "tokens = reduce_to_stems(tokenize(text))\n",
    "print_table(Counter(tokens).most_common(20),['token', 'frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate the stopwords as they do not describe the content of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_stopwords(tokens):\n",
    "    return [token for token in tokens if not(token in stopwords['english'])]\n",
    "\n",
    "tokens = tokenize(text)\n",
    "count = len(tokens)\n",
    "tokens = reduce_to_stems(eliminate_stopwords(tokens))\n",
    "print(f'{count-len(tokens)} stopwords removed ({(count-len(tokens))/count*100:.2f}%)')\n",
    "print(f'{len(tokens)} non-stopword tokens remain with {len(set(tokens))} unique tokens')\n",
    "print_table(Counter(tokens).most_common(20),['token', 'frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We describe each page separately and treat them as mini-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [reduce_to_stems(eliminate_stopwords(tokenize(text))) for text in pages]\n",
    "\n",
    "n = 10\n",
    "print_table(\n",
    "    [\n",
    "        [\n",
    "            i+1,\n",
    "            \" \".join(collection[i])\n",
    "        ] for i in range(n)\n",
    "    ],\n",
    "    [\"page\", \"tokens\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-of-words summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_of_words(tokens):\n",
    "    return set(tokens)\n",
    "\n",
    "n = 10\n",
    "print_table(\n",
    "    [\n",
    "        [\n",
    "            f\"{i+1}\",\n",
    "            \", \".join(sorted(set_of_words(collection[i])))\n",
    "        ] for i in range(n)\n",
    "    ],\n",
    "    [\"page\", \"set of words\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokens):\n",
    "    return dict(Counter(tokens))\n",
    "\n",
    "n = 10\n",
    "print_table(\n",
    "    [\n",
    "        [\n",
    "            f\"{i+1}\",\n",
    "            \", \".join([f'{x[0]}:{x[1]}' for x in sorted(bag_of_words(collection[i]).items())])\n",
    "        ] for i in range(n)\n",
    "    ],\n",
    "    [\"page\", \"bag of words\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words require document frequency and idf weigths for each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(N, df):\n",
    "    return math.log10((N + 1) / (df + 1))\n",
    "\n",
    "terms = defaultdict(int)\n",
    "for page in collection:\n",
    "    # go through each distinct term on this page\n",
    "    for term in set(page):\n",
    "        terms[term] += 1\n",
    "vocabulary = {term: {\"df\": count, \"idf\": idf(len(collection), count)} for term, count in terms.items()}\n",
    "\n",
    "n = 20\n",
    "sample = sorted(random.sample(list(vocabulary.items()), n), key=lambda x: x[1][\"idf\"], reverse=True)\n",
    "print_table(\n",
    "    [\n",
    "        [x[0], f'{x[1][\"df\"]} / {len(collection)}', f'{x[1][\"idf\"]:.3f}'] for x in sample\n",
    "    ],\n",
    "    [\"Term\", \"df\", \"idf\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can compute the bag-of-word representation for vector space retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def bag_of_words_idf(tokens, vocabulary):\n",
    "    terms = dict(Counter(tokens))\n",
    "    return map(lambda w: (w[0], w[1] * vocabulary[w[0]]['idf']), terms.items())\n",
    "\n",
    "n = 10\n",
    "print_table(\n",
    "    [\n",
    "        [\n",
    "            f\"{i+1}\",\n",
    "            \", \".join([f'{x[0]}:{x[1]:.2f}' for x in sorted(bag_of_words_idf(collection[i], vocabulary))])\n",
    "        ] for i in range(n)\n",
    "    ],\n",
    "    [\"page\", \"bag of words (vector space retrieval)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
